<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-18T14:15:34+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chew’s Everyday Blog</title><subtitle>A blog of a CS student.</subtitle><author><name>Chew Y. Feng</name></author><entry><title type="html">Test PDF</title><link href="http://localhost:4000/misc/2025/06/18/pdf-test.html" rel="alternate" type="text/html" title="Test PDF" /><published>2025-06-18T00:00:00+10:00</published><updated>2025-06-18T14:08:17+10:00</updated><id>http://localhost:4000/misc/2025/06/18/pdf-test</id><content type="html" xml:base="http://localhost:4000/misc/2025/06/18/pdf-test.html"><![CDATA[<object data="{{ site.url }}{{ site.baseurl }}/pdfs/abstract-interpreter-compositional-semantics/main.pdf" width="1000" height="1000" type="application/pdf"></object>]]></content><author><name>Chew Y. Feng</name></author><category term="Misc" /><category term="Misc" /><summary type="html"><![CDATA[How can i let it works.]]></summary></entry><entry><title type="html">WSL – File System, Networking, Configuration</title><link href="http://localhost:4000/os/2025/02/22/WSL-FS-Network-Conf.html" rel="alternate" type="text/html" title="WSL – File System, Networking, Configuration" /><published>2025-02-22T00:00:00+11:00</published><updated>2025-02-25T11:37:25+11:00</updated><id>http://localhost:4000/os/2025/02/22/WSL-FS-Network-Conf</id><content type="html" xml:base="http://localhost:4000/os/2025/02/22/WSL-FS-Network-Conf.html"><![CDATA[## Background

I’m a Windows user, and Windows Subsystem Linux (WSL) is the most frequent tool I need to use. 
I use WSL primarily for coding and dev, while Windows for entertaining.
My goal is **not installing anything related to coding in Windows**.
However, I find that I have little knowledge about how it works.
It sometimes makes me confused. 
So I decided to spend sometime and take a note about it.

## Overview

WSL 2 uses virtualization to run a Linux kernel inside a VM.
WSL 2 features
- Faster File IO
- Full System Call Compatibility
		This allows full access to Linux apps like Docker (why) 

Architecture Change from WSL 1
- WSLv1 uses a translation-based approach, which translates a Linux system call to Windows kernel. 
		This approach can be less compatible because system calls in win and Linux can have different semantics. 
		It also slows down the IO speed somehow.
- WSLv2 is VM-based approach. It has a full Linux kernel. 

## Working with File System

You can run Linux tools from a Windows Command Line

```bash
#powershell
wsl ls -la
wsl ls -la "/mnt/c/Program Files" # note that you need to use linux file path  
```

You can also run Windows tools in WSL by `[tool-name].exe` 

```shell
#bash
ipconfig.exe | grep IPv4 | cut -d: -f2
```

## Configuration

WSL allows user providing a configuration file `%UserProfile%/.wslconfig`. 
See [Main WSL Settings](https://learn.microsoft.com/en-us/windows/wsl/wsl-config#main-wsl-settings) for the full configuration table.

By the way, I prefer using WSL setting app for config. 

## Networking Consideration

Currently, WSL2 offers two modes of networking: NAT and Mirrored.

### NAT Mode

NAT Mode means that WSL is running in a **virtualized network environment** where it is assigned with a **private IP address**, and Windows serves as the adapter. 

**Accessing Internet from WSL**

WSL runs on NAT mode implies that Windows serves as the adapter, rewriting request from WSL with its IP. 

Try this:
```
> wsl curl 'https://api.ipify.org?format=json'
> curl 'https://api.ipify.org?format=json'
```

The API simply returns the IP of the client. 
You can see that both are identical and is the IP address of the Windows.

Another experiment is that, you can trace network packets:
```bash
traceroute google.com # On wsl
```

```bash
tracert google.com # On wsl
```

You will find that both are identical **except there is one more jump from WSL to Windows** 

**Accessing WSL from Windows**

On Windows, you can access networking apps in WSL using localhost. There is an auto-forwarding feature working: 

![Experiment: Accessing WSL Networking application ](/assets/images/Pasted%20image%2020250222170500.png)

You can also access networking apps by using the assigned IP address of WSL. 

The Windows host can use this command to query the IP address 
```powershell
> wsl -d <DistributionName> hostname -I
```

**Accessing WSL from LAN**

To enable devices on your LAN to access WSL, it needs proper proxy setup. See [Accessing a WSL 2 distribution from your local area network (LAN)](https://learn.microsoft.com/en-us/windows/wsl/networking#accessing-a-wsl-2-distribution-from-your-local-area-network-lan)

### Mirrored Networking Mode

In mirrored mode, WSL works as **if they are the same machine in network**. Benefits are:
- IPv6 Support
- Connect to Windows servers from within Linux using the localhost address `127.0.0.1`.
- Improved networking compatibility for VPNs
- Connect to WSL directly from your local area network (LAN)

It implies that both Windows and WSL Apps connect to the same physical network of the Windows host. 

The VM “mirrors” all network settings from Windows. 

This can be validate by tracing network parcels:

```
# on WSL
traceroute google.com
```

You will find there is not a jump to Windows host. 

The only thing we know about the implementation is that it utilizes Hyper-V virtual switch feature. 
MS do not share more detailed information about it.

## Mounting a Device

> Shout out to [https://askubuntu.com/questions/1116200/how-can-i-access-my-usb-drive-from-my-windows-subsystem-for-linux-ubuntu-dist]

Suppose there is a new USB device mounted on G: on windows. 
To access it on WSL: 

1. Create the mount point: `mkdir /mnt/g` 
2. Mount the drive to the directory using `sudo mount -t drvfs G: /mnt/g`


# References
1. MS WSL Doc https://learn.microsoft.com/en-us/windows/wsl/about
2. Intro to WSL2 Video https://youtu.be/MrZolfGm8Zk?si=pRhVjzF84i49Zoqi]]></content><author><name>Chew Y. Feng</name></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[My story with Linux subsystem.]]></summary></entry><entry><title type="html">Simply Typed Lambda-Calculus</title><link href="http://localhost:4000/software-foundations/2024/07/18/Simple-Typed-Lambda-Calculus.html" rel="alternate" type="text/html" title="Simply Typed Lambda-Calculus" /><published>2024-07-18T00:00:00+10:00</published><updated>2024-07-18T17:28:57+10:00</updated><id>http://localhost:4000/software-foundations/2024/07/18/Simple-Typed-Lambda-Calculus</id><content type="html" xml:base="http://localhost:4000/software-foundations/2024/07/18/Simple-Typed-Lambda-Calculus.html"><![CDATA[This article is a study note of [Programming Language Foundation](https://softwarefoundations.cis.upenn.edu/plf-current/Stlc.html)

## Overview

The simply typed lambda calculus (STLC) is a wonderful study target which represents many intriguing language features among modern programming language.

Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution (From Wikipedia). 

Consider a lambda-calculus system that only contains Boolean values, which can be expressed by following BNF notation:

```
t ::= x                      (variable)
 | \x:T,t                    (abstraction)
 | t t                       (application)
 | true                      (constant true)
 | false                     (constant false)
 | if t then t else t        (conditional)
```

Examples:

```c
\x: Bool, x // identity function
(\x: Bool, x) true // apply the identity function to value true
\x:Bool, if x then false else true // function 'not'
\f:Bool→Bool, f (f true) // a higher order function
```

Although it only contains Boolean values, it processes the computation ability that equivalent any modern programming languages. It even supports higher order function that some “modern” languages don’t support well, like C and C++.  

Note that there are variables involved in the definition. A *complete* program is a program that never refer to any undefined variables. In most time, we want the program to be complete and we can simply fail there are free variables (which doesn’t bind to any terms and types).  
## Operational Semantics

### Values

To decide what are the *values* of STLC, a little discussion is needed on the case of abstraction. An abstraction (similar to a “function” in imperative programming language) is of such form

```
\x:T, t
```

There are two choices:
1. An abstraction is always a value
2. An abstraction is a value if `t` is a value

I would say I prefer option 1, and that is the choice in the textbook, because it makes things a lot easier. But option 2 is also the choice for some language, for some reasons like compile-time optimization.

### Substitution

The heart of lambda calculus is substitution. For example, we reduce

```
 (\x:Bool, if x then true else x) false
```

to 

```
if false then true else false
```

The application works by substitute `\x` into the bind value `false`. We mark the substitution as `[x:=s]t`, which literally says “substitute all occurrences of `x` into `s` in the term `t`”.

More example:
- `[x:=true] (if x then x else y)` yields `if true then true else y`
- `[x:=true](\y:Bool,x)` yields `\y:Bool,true`

Substitution is a deterministic and easy problem (i.e. not a NP-hard one). So the textbook simply encoded substitution into a fixpoint function `subst`. It’s very interesting to think how a powerful computation model is just such a small substitution machine.

Exercise: 3 stars, standard (substi_correct)
This Exercise asks us to define substitution as a inductive relation `substi` , and prove the relation is equivalent to the fixpoint version.  

```coq
Inductive substi (s : tm) (x : string) : tm -> tm -> Prop

Theorem substi_correct : forall s x t t',
  <{ [x:=s]t }> = t' <-> substi s x t t'.
```

- For direction `->`, it can be proved by induction on term `t`
- For direction `<-`, it can be proved by induction on relation `substi`  

### Reduction

Based on substitution, we can define how an expression is *reduced* to value. The core of reduction is the application rule, which says 

```
(\x:T,t) v --> [x:=v] t
```

To make the process of reduction to be deterministic, we made a design: 
**Apply reduction of application expression only if both the left-hand side (the abstraction) and the right-hand side (the argument) is a value.**

## Typing

The STLC has a powerful feature – typing, which becomes more and more important in programming language design. For some reasons, computer scientists write typing relation as following form:

```
|-- t \in T
```

which says that “term t has type T”. 

An obvious problem is typing abstractions. Consider such abstraction:

```
\x:T1, t2
```

which has type `T1 -> T2`. If we want to typecheck it, we need to make sure term `t2` has type `T2` **under the context that `x` has type `T1`.**

Context can be represented as a single global information, which adds to the typing relation. A map (partial-map) is capable of representing such information, which we name it *Gamma*. The new typing judgment is written `Gamma |-- t \in T` and informally read as "term t has type T, given the types of free variables in t as specified by Gamma". 

Examples:

```coq
Example typing_example_1 :
  empty |-- \x:Bool, x \in (Bool → Bool).
```

Finally, we present the typing rule of abstraction:

$$
\frac{x \mapsto T2 ; \Gamma \vdash t1 \in T1}{
\Gamma \vdash \backslash x:T2,t1 \in T2\to T1
}
$$
Other rules can be found in the textbook as well.

Talking about variables, a term is *closed* if all variables in it are well typed (contain no free variables).

## Properties of STLC

We discuss what makes the language to be “well and sound”. 

### Canonical Form

We can establish the relation between well-typed values (canonical form)  and their type. For example, if a value has type `Bool`, it must be a `false` or `true`. 
### Progress

The progress theorem tells us that closed, well-typed terms are not stuck: either a well-typed term is a value, or it can take a reduction step.

```coq
Theorem progress : ∀ t T,
  empty |-- t \in T →
  value t \/ ∃ t', t --> t'.
```

Exercise: 3 stars, advanced (progress_from_term_ind). 

It asks us to prove theorem `progess` by induction on terms. The proof is a bit long but not complicated. Just follow the reduction rules to see if it can make a step, or it is a value, or there is a contradiction in hypothesis. 

### Preservation

The preservation theorem says reduction doesn’t change the type of a term (so it *preserves* the type). This property shows our typing is indeed sound and well-fitted into the operational semantics. 

To establish the preservation property on STLC, we take a few steps:
1. The Weakening Lemma, which says if `t` has type `T` in some context `Gamma`and  `Gamma` is included in a bigger context `Gamma'`, t also has type `T` in `Gamma'`.
	To prove this, one need to consider what “a context is included in other context” means. But it’s fairly straightforward.
2. The Substitution Lemma, which says substitution preserves the type. 
3. The main theorem. With the help of substitution lemma, it can be proved with ease.

Exercise: 3 stars, advanced (substitution_preserves_typing_from_typing_ind)
It asks us to prove the substitution lemma, which lies at the heart of this proof chain. A few tips:
- Read carefully about the informal proof provided by the author. 
- When encounter `(x =? y)%string` (string equality), use `destruct eqb_spec` to perform case analysis.
- When the goal is the form `Gamma |-- t \in T`, you can use the weakening lemma to transform it to `empty |-- t \in t`.

Exercise: 2 stars, standard, especially useful (subject_expansion_stlc)
Show that the reverse to preservation theorem does not hold. That is, if `t --> t'` and `t' \in T`, we can not make sure `t \in T`.
I exploit the fact that reduction doesn’t type check.
```
(* t *) exists <{ (\x: Bool -> Bool, x) true }>.
(* t'*) exists <{ true }>.
(* T *) exists <{ Bool }>.
```
It’s a little bit cheating, I admit. But I can’t find a better one. 
### Type Soundness

Put progress and preservation together, we can show that a well-typed term can _never_ reach a stuck state.

Exercise: 2 stars, standard, optional (type_soundness)
Type soundness can be proved by induction on Hmulti (`t -->* t'`).

### Uniqueness of Types

Another nice property of the STLC is that types are unique: a given term (in a given context) has at most one type.

Exercise: 3 stars, standard (unique_types)
This exercise can be solved by induction on term `e`. The whole proof heavily use inversion to do case analysis. 

## Extension to STLC

One might argue that the current STLC is still too simple and cannot match a real modern programming language. Well, there are a lot of extensions to STLC in the textbook to show the possibility of the language.

- Numbers. 
- Let bindings, which adds the let expression like those in Haskell.
- Pairs, which are popular in many languages. For example, I can have a pair of `(1, false)` of type `(Nat, Bool)` 
- Unit. A unit type is a type that only has one value. It can be quite helpful to express things like `null`, `nullptr`
- Sums. A sum is like the `Either` monads In Haskell. 
- Lists. 
- Recursion (Recursive Abstraction)
- Records. Like dictionary in python.

The exercises in this chapter is to formalize these features, including the substitution, reduction and typing. I would say it is quite straightforward to follow the informal definition given by the author, and satisfying to see how it works.]]></content><author><name>Chew Y. Feng</name></author><category term="Software-Foundations" /><category term="Coq" /><category term="PLT" /><summary type="html"><![CDATA[A small yet powerful language]]></summary></entry><entry><title type="html">Small-Step Operational Semantics</title><link href="http://localhost:4000/software-foundations/2024/06/22/Small-Steps.html" rel="alternate" type="text/html" title="Small-Step Operational Semantics" /><published>2024-06-22T00:00:00+10:00</published><updated>2024-06-22T15:27:12+10:00</updated><id>http://localhost:4000/software-foundations/2024/06/22/Small-Steps</id><content type="html" xml:base="http://localhost:4000/software-foundations/2024/06/22/Small-Steps.html"><![CDATA[Even if I have walked through all the content and practices in the [textbook](https://softwarefoundations.cis.upenn.edu/plf-current/Smallstep.html), it still confuses me about what “Small-Step Operational Semantics” is. It is partly because the study of the chapter is segmented into pieces because I have to only use my free time to do it. It also because this chapter is way too long such that one can’t get to the important points immediately.

## Small-step Semantics

*Small-step* means the evaluation of the semantics from the program can be carried out step-by-step. Some would also describe it as “structural operational semantics” because the the method usually follows the structure of programs. It might takes multiple steps to get the final semantics of a statement. Conversely, *natural semantics* directly evaluate a statement to the final state, which can be steadily implemented in a evaluation function.

For example, given the statement

```
(1 + 3) + (2 + 4)
```

It might takes multiple steps to get the semantics:

```
(4) + (2 + 4)
(4) + (6)
10 <-- final semantics
```

There are two reasons (the textbook gives) to recommend small-step semantics instead of natural semantics.
- We includes the intermediate states that it passes through along the way. Observable execution is critical in many situation. 
- By only define one-step evaluation, we allow *undefined behaviours* in the program. 

## Implementation Comparison

Compared to natural semantics, the implementation of small-step would be more complicated – a single function can’t suffice. Fortunately we are doing this in Coq, which has a very power tool  – Inductive type. 

In the most naïve language where there are only constant (denotes by C) and addition (denotes by P), the informal semantics are: 

$$\frac{}{C~n \to n}$$

$$\frac{t_1 \to n_1,~t_2\to n_2}{P~t_1~t_2 \to n_1 + n_2}$$

The formal semantics can be encoded in Coq:

```coq
Inductive tm : Type :=
  | C : nat → tm (* Constant *)
  | P : tm → tm → tm. (* Plus *)
```

The natural semantics is a function defined in a very straightforward way

```coq
Fixpoint evalF (t : tm) : nat :=
  match t with
  | C n ⇒ n
  | P t1 t2 ⇒ evalF t1 + evalF t2
  end.
```

This evaluation function can easily handle statement such as:
- `C 3`
- `P (C 3) (C 5)`
- `P (C 3) (P (C 2) (C 3))`

Conversely, the small step semantics involves a inductive relation describing how to take a step forward:

```coq
Inductive step : tm → tm → Prop :=
  | ST_PlusConstConst : ∀ n1 n2,
      P (C n1) (C n2) --> C (n1 + n2)
  | ST_Plus1 : ∀ t1 t1' t2,
      t1 --> t1' →
      P t1 t2 --> P t1' t2
  | ST_Plus2 : ∀ n1 t2 t2',
      t2 --> t2' →
      P (C n1) t2 --> P (C n1) t2'

  where " t '-->' t' " := (step t t').
```

And another inductive relation which defines a multi step evaluation as multiple consecutive single-step evaluations:

```coq
Inductive multi {X : Type} (R : relation X) : relation X :=
  | multi_refl : ∀ (x : X), multi R x x
  | multi_step : ∀ (x y z : X),
                    R x y →
                    multi R y z →
                    multi R x z.
```

This example shows the difference. You can clearly see that small-step semantics is quite complicated compared to natural semantics, which is only a function. 

## Properties of Small-step Semantics

### Deterministic

Firstly, given two same programs, the semantics that we found should be identical. In other word, there should be no randomness get in the way for this tiny toy language. This property is called *deterministic*.  

**Deterministic Property**: “for each t, there is at most one t' such that t steps to t' (t --> t' is provable).”

It sounds intuitive but the proof is not so straightforward. Readers can check the textbook `Theorem step_deterministic` if interested. 
### Values

There is a slight problem. When dealing with expressions such as

```
P (P (C 1) (C 3)) (P (C 2) (C 4))
```

There are actually two directions to step forward: – either handle 1 + 3 or 2 + 4, which results in different intermediate states. This is not we typically want and sometimes causes trouble. 

```haskell
P (P (C 1) (C 3)) (P (C 2) (C 4))
P (C 4) (P (C 2) (C 4))
P (C 4) (C 6)
C 10
-- or
P (P (C 1) (C 3)) (P (C 2) (C 4))
P (P (C 1) (C 3)) (C 6)
P (C 4) (C 6)
C 10
```

This problem can be solved by defining *values*. We always want the final term to be some special forms, which we called *values*. In this language, we expect the final semantics to some constant terms.

```coq
Inductive value : tm → Prop :=
  | v_const : ∀ n, value (C n).
```

With the concepts of value, we revise the definition of steps:

$$\frac{t_1 \to t_1',}{P~t_1~t_2 \to P~t_1'~t_2}$$

$$\frac{\text{value}~t_1,~t_2\to t_2'}{P~t_1~t_2 \to P~t_1~t_2'}$$

The second one is crucial. It says we take the step of $t_2$ only when t1 is a value. This definition ensures we always evaluate the first argument (t1) until it reaches the form of value. 

The usage of values doesn’t limit to this. In fact, they are very important and critical in small-step semantics. 

### Progress and Normal Forms

In a large language, sometimes people can easily forget one or more rules, which results in a incomplete definition. One should prove its semantics to be *strong progress* to show there is nothing left. 

**Theorem Strong Progress**: If t is a term, then either t is a value or else there exists a term t' such that t-->t'.

When there are statement that doesn’t make sense, we don’t want to define every possible combination of terms. For example, if there are lists in the language, how could `1 + []` proceeds? The solution is called *typed progress*, which involves defining type systems to distinguish what statement doesn’t make sense, and only well-typed terms are progress. We don’t discuss more about types in this article.

Terms that cannot make progress are called *normal forms*. In this language, only values cannot make a further step. This is quite something that suggests our definition tend to be correct. Why? Because value is a syntactic concept – it is defined by looking at the way a term is written – while normal form is a semantic one – it is defined by looking at how the term steps. 

## Multi-Step Reduction

Recall how multi-step evaluation relation is defined:

```coq
Inductive multi {X : Type} (R : relation X) : relation X :=
  | multi_refl : ∀ (x : X), multi R x x
  | multi_step : ∀ (x y z : X),
                    R x y →
                    multi R y z →
                    multi R x z.
```

This relation has a few properties
 - It is obviously _reflexive_ (from the rule `multi_refl`).
 - In the second rule, it takes a step relation $R$ to define what is multi-step evaluation of $R$. This is called *closure* of *R*. 
 - Third, it is *transitive*. i.e. If `multi t1 t2` and `multi t2 t3`, we have `multi t1 t3`

### Normal Forms in Multi-Steps

If t reduces to t' in zero or more steps and t' is a normal form, we say that "t' is a normal form of t."

```coq
Definition normal_form_of (t t' : tm) :=
  (t -->* t' ∧ step_normal_form t').
```

This definition is important because it define what is typically the end of the evaluation. 

We have seen single-step evaluation is deterministic. Following that, we also want to ensure that if t can reach a normal form, then this normal form is unique.

## Conclusion

This article explains what is small-step operational semantics and what are the properties that developers should be aware of.]]></content><author><name>Chew Y. Feng</name></author><category term="Software-Foundations" /><category term="Coq" /><category term="PLT" /><summary type="html"><![CDATA[Finding the meaning of a program]]></summary></entry><entry><title type="html">Distributed Transactions</title><link href="http://localhost:4000/coursenote/2024/05/11/Distributed-Transactions.html" rel="alternate" type="text/html" title="Distributed Transactions" /><published>2024-05-11T00:00:00+10:00</published><updated>2024-05-11T16:45:56+10:00</updated><id>http://localhost:4000/coursenote/2024/05/11/Distributed-Transactions</id><content type="html" xml:base="http://localhost:4000/coursenote/2024/05/11/Distributed-Transactions.html"><![CDATA[In a distributed DBMS, a transaction usually involves multiple servers. Consider a transaction that a client transfers 10 dollars from account A to C and then transfers 20 dollars from B to D, and each of these accounts is stored in different servers as shown in the graph. If any one server in this transaction fails, the whole transaction needs to abort. 

![DistributedTransaction](/assets/images/Pasted image 20240511143920.png)

A client transaction becomes distributed if it invokes operations in several different servers. This document talks about the main strategies to achieve ACID properties for distributed transactions.

## Two-Phase Commit Protocol

The two-phase commit protocol (2PC) was devised to ensure the atomicity property in a distributed DBMS. where the atomicity property requires that when a distributed transaction comes to an end, **either all of its operations are carried out or none of them**.

<!-- ![[Pasted image 20240511143920.png]] -->

In 2PC, firstly, a server is selected to be the coordinator, which communicates with the client and coordinate the work on all servers. 

![DistributedTransaction](/assets/images/Pasted image 20240511144355.png)
<!-- ![[Pasted image 20240511144355.png]] -->

In the first phase of 2PC, each participant votes for the transaction to be committed or aborted. Once a participant has voted to commit a transaction, it is not allowed to abort it. Therefore, before a participant votes to commit a transaction, **it must ensure that it will eventually be able to carry out its part of the commit protocol**, even if it fails. A participant to said to be in a *prepared* state if it votes to commit. 

In the second phase of the protocol, the coordinator decides the final decision (either abort or commit), every participant in the transaction carries out the joint decision. 

To formally define the protocol, we first list the operations (interfaces) in the protocol:
- *canCommit*(trans) Yes / No
	- Call from coordinator to participant to ask whether it can commit a transaction. Participant replies with its vote.
- *doCommit(trans) 
	- Call from coordinator to participant to commit its part of a transaction.
- *doAbort*(trans) 
	- Call from coordinator to participant to abort its part of a transaction.
- *haveCommitted*(trans, participant) 
	- Call from participant to coordinator to confirm that it has committed the transaction.
	- Just for deleting stale information on the coordinator.
- *getDecision*(trans) -> Yes / No
	- Call from participant to coordinator to ask for the decision on a transaction after it has voted Yes but has still had no reply after some delay. Used to recover from server crash or delayed messages.

Phase 1 
1. The coordinator sends a *canCommit*? request to each of the participants in the transaction. 
2. When a participant receives a *canCommit*? request it replies with its vote (Yes or No) to the coordinator. Before voting Yes, it prepares to commit by saving objects in permanent storage. If the vote is No, the participant aborts immediately.

Phase 2
1. The coordinator collects the votes (including its own). 
	1. If there are no failures and all the votes are Yes, the coordinator decides to commit the transaction and sends a *doCommit* request to each of the participants. 
	2. Otherwise, the coordinator decides to abort the transaction and sends *doAbort* requests to all participants that voted Yes. 
2. Participants that voted Yes are waiting for a *doCommit* or *doAbort* request from the coordinator. When a participant receives one of these messages it acts accordingly and, in the case of commit, makes a *haveCommitted* call as confirmation to the coordinator.

![DistributedTransaction](/assets/images/Pasted image 20240511145431.png)
<!-- ![[Pasted image 20240511145431.png]] -->

Timeout is employed in 2PC.
- If a participant votes yes, but not receiving any reply from the coordinator after a certain time, it enters the *uncertain* state. It can make a *getDecision* request to the coordinator to determine the outcome. 
- Since after a participant votes yes, it can’t do anything until receive a decision from the coordinator. Thus, if a coordinator fails, it must be replaced. 
- Conversely, if a participant can’t vote in a certain time, the coordinator must announce *doAbort* to all participants.

## Timestamp Ordering

Similarly, the isolation property needs to be preserved in distributed transactions. There are two meanings behind isolation in distributed DBMSs:
1. Each server needs to be responsible for applying concurrency control to its own objects.
2. All members of a distributed transactions are jointly responsible for ensuring that they are performed in a serially equivalent manner.

We mainly talk about 2 in this article, where “serially equivalent manner” or “serializability” means if transaction T is before transaction U in their conflicting access to objects at one of the servers, then they must be in that order at all of the servers whose objects are accessed in a conflicting manner by both T and U.

The most common protocol is the timestamp ordering. in the protocol, similarly, a central coordinator is selected. 
- The coordinator issues a timestamp – the globally unique transaction timestamps – to each transaction when it starts to participants. All members are responsible for ensuring that transactions execute in the order of timestamp.
- The transaction timestamp is passed to the coordinator at each server whose objects perform an operation in the transaction. In this way, the coordinator can find the potential inconsistency

## One-copy Serializability

There is a final concerns of distributed transactions. Usually, for increasing data availability, there are replicas for objects. From a client’s viewpoint, a transaction on replicated objects should appear the same as one with non-replicated objects. This property is named *one-copy serializability*. It is similar to, but not to be confused with, sequential consistency or sequential serializability.

The simplest protocol to achieve one-copy serializability is **read-one/write-all**. As its name suggests, when it reads, it can read from any one of the replicas. But a write requests must be performed on all replicas by the replica manager. 

![DistributedTransaction](/assets/images/Pasted image 20240511161255.png)
<!-- ![[Pasted image 20240511161255.png]] -->

Simple read-one/write-all replication is not a realistic scheme, because it cannot be carried out if some of the replica managers are unavailable. This deficiency essentially contradicts with the purpose of replicas because replicas scheme is designed to allow for some replica managers being temporarily unavailable. 

A more realistic protocol is local validation. It allows a transaction to write only to available replicas. However, before a transaction commits it checks for any failures (and recoveries) of replica managers of objects it has accessed. A transaction can only commits if there is no failure and recovery of replicas which stores the object used in the transaction.

## Reference

Coulouris, George F., Jean Dollimore, and Tim Kindberg. Distributed systems: concepts and design. pearson education, 2005.]]></content><author><name>Chew Y. Feng</name></author><category term="CourseNote" /><category term="Database" /><summary type="html"><![CDATA[Deal with the complexity in distributed systems.]]></summary></entry><entry><title type="html">Transaction Management</title><link href="http://localhost:4000/coursenote/2024/04/19/Transaction-Management.html" rel="alternate" type="text/html" title="Transaction Management" /><published>2024-04-19T00:00:00+10:00</published><updated>2024-05-11T16:41:14+10:00</updated><id>http://localhost:4000/coursenote/2024/04/19/Transaction-Management</id><content type="html" xml:base="http://localhost:4000/coursenote/2024/04/19/Transaction-Management.html"><![CDATA[The document summarize transaction management in database.

Content from Unimelb Master Subject [COMP90050](https://handbook.unimelb.edu.au/subjects/comp90050)

## Background

Some cliché here:
Transaction is a **unit of work** in a database
Properties of transaction: **ACID**
- **Atomicity**. Either all operations of the transaction are reflected properly in the database, or none are
- **Consistency**. Execution of a transaction in isolation (i.e., with no other transaction executing concurrently) preserves the consistency of the database.
	- What is “consistent” often depends on applications
	- Not easily computable in general
- **Isolation**. Even though multiple transactions may execute concurrently, the system guarantees that, for every pair of transactions, it appears that one is unaware of another
	- mainly about concurrency control
- **Durability**. After a transaction completes successfully, the changes it has made to the database persist, even if there are system failures.

Not all operations can have ACID. Three types of operations in DBMS
- Unprotected actions. actions with no ACID. e.g. low level read and write from memory / disk
- **Protected actions**
- Real actions. These actions affect the real, physical world in a way that is hard or impossible to reverse. theoretically impossible to implement ACID. e.g. printing a report, sending a SMS message…

## Transaction from Programmers Perspective

Not all queries can be expressed in SQL, so a transaction often works with a general purpose language such as C and Java

Two approaches to accessing SQL: Dynamic SQL and Embedded SQL
- Dynamic SQL ← construct and submit SQL queries at **runtime**. e.g. JDBC
- Embedded SQL ← compile SQL statement at **compile time**. e.g. C Embedded SQL

The main distinction between Dynamic SQL and Embedded SQL is the existence of **Pre-processing**. Preprocessing can catch errors in the compile stage e.g. type error, SQL syntax error, but it complicates debugging of the program and even causes ambiguity because it, actually creates a new host language. As a result, most modern systems use dynamic SQL. 

Finally, note that nondeclarative actions in programs – printing to terminal, sending result to UI – cannot be undone by DB
### C Embedded SQL 
Example a flat transaction.

First, create a function with no transaction

```c
int main() {
	exec sql BEGIN DECLARE SECTION;
	/* The following variables are used for communicating between SQL and C */
	int OrderID; 		/* Employee ID (from user) */ 
	int CustID;	 	/* Retrieved customer ID */ 
	char SalesPerson[10] /* Retrieved salesperson name */ 
	char Status[6] 	/* Retrieved order status */ 
	exec sql END DECLARE SECTION; 
	
	/* Set up error processing */
	exec sql WHENEVER SQLERROR GOTO query_error;
	exec sql WHENEVER NOTFOUND GOTO bad_number;
	
	/* Prompt the user for order number */
	printf ("Enter order number: ");
	scanf_s("%d", &OrderID);
	
	/* Execute the SQL query, simple query no transaction definition */
	exec sql SELECT CustID, SalesPerson, Status
	FROM Orders
	WHERE OrderID = :OrderID;// ”:” indicates to refer to  C variable
	INTO :CustID, :SalesPerson, :Status;
	
	printf ("Customer number: %d\n", CustID);
	printf ("Salesperson: %s\n", SalesPerson);
	printf ("Status: %s\n", Status);
	
	exit();
	
	query_error:
	printf ("SQL error: %ld\n", sqlca->sqlcode); exit();
	
	bad_number:
	printf ("Invalid order number.\n"); exit(); 
}
```

Then, put transaction into the example: Everything between 
```c
exec sql BEGIN WORK;
```
and
```c
exec sql COMMIT WORK;
```
is seen as transaction. 

e.g. 
```c
DCApplication() {
	exec sql BEGIN WORK;
	AccBalance = DodebitCredit(BranchId, TellerId, AccId, delta);
	// Consistency Check
	if (AccBalance < 0 && delta < 0) {
	// Inconsistent
	// Let dbms undo whatever it has done
		exec sql ROLLBACK WORK;
	} else { 
		send output msg;
		exec sql COMMIT WORK;
	}
}

Long DoDebitCredit(long BranchId, long TellerId, long AccId, long delta) {
	exec sql UPDATE accounts
	SET AccBalance =AccBalance + :delta
	WHERE AccId = :AccId;
	exec sql SELECT AccBalance INTO  :AccBalance
	FROM accounts WHERE AccId = :AccId;
	exec sql UPDATE tellers
	SET TellerBalance = TellerBalance + :delta
	WHERE TellerId = :TellerId;
	exec sql UPDATE branches
	SET BranchBalance = BranchBalance + :delta
	WHERE BranchId = :BranchId;
	
	Exec sql INSERT INTO history(TellerId, BranchId, AccId, delta, time)
	VALUES( :TellerId, :BranchId, :AccId, :delta, CURRENT);
	
	return(AccBalance);
}
```

the transaction will either survive together with everything from BEGIN WORK to COMMIT WORD, or it will be rolled back with everything (abort).

### Dynamic SQL

Take JDBC as an example.

Mainly four steps:
- Open a connection
- Create a statement object
- Execute queries using the statement object to send queries and fetch results
- Exception mechanism to handle errors

```java
public static void JDBCexample(String userid, String passwd)
{
	try (
		// Open a connection
		Connection conn = DriverManager.getConnection(
			"jdbc:oracle:thin:@db.yale.edu:1521:univdb",
			userid, passwd);
		// Create a statement object
		Statement stmt = conn.createStatement();
	) {
		try {
			// Execute queries using the statement object
			stmt.executeUpdate(
			"insert into instructor values(’77987’,’Kim’,’Physics’,98000)");
		}
		catch (SQLException sqle) {
			System.out.println("Could not insert tuple. " + sqle);
		}
		// Execute queries using the statement object
		ResultSet rset = stmt.executeQuery(
			"select dept_name, avg (salary) "+
			" from instructor "+
			" group by dept_name");
		while (rset.next()) {
			System.out.println(rset.getString("dept_name") + " " +
			rset.getFloat(2));
		}
	}
	catch (Exception sqle) {
		// handle exception
		System.out.println("Exception : " + sqle);
	}
}
```

*Automatic Commit*. Most DBs treats each SQL statement as a transaction by default. 

To create a transaction, automatic commit must be turn off.

```java
assert conn instanceof Connection;
conn.setAutoCommit(false);
```

A transaction can be put between 
```java
conn.commit(); 
stmt.executeUpdate(...);
conn.rollback();
```

## Savepoints And Nested Transactions

The examples above are flat transactions.

A *flat transaction* typically refers to a transaction that doesn't involve multiple steps or subtransactions. It's a simple, single-operation transaction where a database operation is executed as a single unit of work.

A typical flat transaction:
```
BEGIN WORK
S1: book flight from Melbourne to Singapore
S2: book flight from Singapore to London
S3: book flight from London to Dublin
COMMIT WORK
```

The semantic of flat transaction is ***“all-or-nothing”***
- A naïve implementation: giving up everything that has been done (invoke `ROLLBACK WORK`)
- Good at short applications such as debit/credit 
- Problem: waste of computation power 

A better solution: stepping back to an earlier state _inside the same transaction_ by **_savepoints_**

![SavePoints](/assets/images/Pasted image 20240407110700.png)

Savepoints are explicitly established by the application program and can be reestablished by invoking a modified ROLLBACK function, which is aimed at an internal savepoint rather than at the beginning of the transaction.

An alternative to savepoints is nested transactions (or subtransactions)

Nested transactions are a generalization of savepoints. Whereas savepoints allow organizing a transaction into a _sequence_ of actions that can be rolled back individually, nested transactions form a _hierarchy_ of pieces of work.

![NestedTransaction](/assets/images/Pasted image 20240407111000.png)

Definition of nested Transaction:

- A nested transaction is a tree of transactions, the sub-trees of which are either nested or flat transactions.
- A subtransaction can either commit or roll back; its commit will not take effect, though, unless the parent transaction commits. By induction, therefore, any subtransaction can finally commit only if the root transaction commits.

Rules of subtransactions:
- **Commit rule.** The commit of a subtransaction makes its results accessible only to the parent transaction. Any subtransaction can finally commit only if the root transaction commits.
- **Rollback rule.** If a transaction at any level is rolled back, all its subtransactions are also  rolled back
- **Visibility rule.** All changes done by a subtransaction become visible to the parent transaction upon the subtransaction’s commit. All objects held by a parent transaction can be made accessible to its subtransactions. Changes made by a subtransaction are not visible to its siblings, in case they execute concurrently. Siblings cannot view each other.

subtransactions are more capable than savepoints + flat transaction:
- subtransactions can run in parallel so more effecient.
- the code will be more organizable

Conclusion: subtransactions have ACI, but no D (The changes are made durable only after the whole transaction has committed)

## Transaction Processing Monitors

_TP monitors_ are the least well-defined software term. They function differently in different DBMS.

The main function of a TP monitor is **to _integrate_ other system components and manage resources**.

The TP monitor integrates different system components to provide a uniform applications and operations interface with the same failure semantics (ACID properties).

In this subject, TP monitors are defined to be
- TP monitors manage the **transfer of data between clients and servers**
- Breaks down applications or code **into transactions and ensures that all the database(s) are updated properly**
- It also takes appropriate **actions if any error occurs**

Services:
- **Terminal(clients) management**. manage connections, deliver and receive message by users
- **Presentation services**. dealt with different UI software, similar to the previous one
- **Context management.**  context such as “Current authenticated userid at that terminal,” “Default terminal for that user,” “Account number for that user,” “Current user role.”…
- **Start/restart**. handle the restart after any failure

### Structures of TP Monitors

**One process per terminal**

Each process can run all applications. Each process may have to access any one of the databases. This design is typical of time-sharing systems. It has many processes and many control blocks.

Very memory expensive, context switching causes problems too..

**Only one terminal process**

In this solution, there is just one process in the entire system. It talks to all terminals, does presentation handling, receives the requests, contains the code for all services of all applications, can access any database, and creates dynamic threads to multiplex itself among the incoming request

This would work under a multithreaded environment but cannot do proper parallel processing, one error leads to large scale problems, not really distributed and rather monolithic

**Many Servers, One Scheduler**

Multiple processes have one requester, which is the process handling the communication with the clients.

**Multiple communication processes and servers**

**Generalization of the coexistence approach: multiple application servers invoked by multiple requesters.** The association between these groups of functionally distinct processes, load control, activation/deactivation of processes, and so on, must now be coordinated by a separate instance, the monitor process.

## Transaction Concurrency Control

Concurrency control guarantee the **isolation** properties

Idea: impose exclusive access to shared variables on different threads.

Approaches:
- Dekker’s Algorithm – needs almost no hardware support. but high complexity
- OS support primitives – simple to use, expensive, dependent on OS
- Spin locks – the most common mechanism in DBMS
	Atomic lock/unlock instructions

**Dekker’s Algorithm**

```c
int turn = 0 ; int wants[2]; 		// both should be initially 0
…
while (true) {
	wants[i] = true; 		// claim desire
	while (wants[j]) {
		if (turn == j) {
			wants[i] = false;
			while (turn == j);
			wants[i] = true;
		}
	}
	counter = counter + 1: 	// resource we want mutex on
	turn = j; 			// assign turn
	wants[i] = false;
}…
```

- Manage *desire* by arrays
- Use loop statement to wait (busy waiting) → waste of computation power
- Transfer turn to others when task is done

**OS Exclusive Access**

OS support primitives as lock and unlock
- Do not use busy waiting, but also very expensive
- also OS dependent

**Spin Locks**

- Executed using atomic machine instructions such as test and set or swap
- Need hardware support ← need to lock bus
- uses busy waiting
- very efficient for low lock contentions, commonly used in DBMS

```c
int lock = 1;
// lock granted
acquire(lock);
while(!testAndSet(&lock)); // busy waiting
counter = counter + 1
// release lock
lock = 1;

testAndSet(int* lock) {
	// Assume this line runs in atomic
	if(*lock == 1) {*lock = 0; return true} else {return false}
}
```

Yet another spin lock – compare and swap

more generic lock mechanism in programming languages:

```java
lock(var);
unlock(var);
```

### Semaphore
Definition: A bit, token, fragment of code, or some other mechanism which is used to restrict access to a shared function or device to a single process at a time, or to synchronize and coordinate events in different processes.

Semaphores derive from the **trains**: a train may proceed through a section of track only if there is no other train on the track. Else, it waits

Implement in computers:
- A general `get` subroutine to acquire access, and a `give` to give access to others
- Put waiting processes into a queue to let them wait (simplest: a linked list)
- After a process release the access, give it to the very next process

Semaphores are simple locks; no deadlock detection, no conversion, no escalation…

Exclusive Semaphores. An exclusive semaphore is a pointer to a linked list of processes, indicating the exclusive ownership of resources. The process at the end of the list own the semaphore.

To acquire a semaphore, a process needs to call `get()`

An example `get()` of an exclusive semaphore
![Semaphore-Get](/assets/images/Pasted image 20240417095100.png)

Note that `CS` stands for “compare-and-swap“ (a spin lock)
```c
boolean cs(int *cell, int *old, int *new)
{/* the following is executed atomically*/
if (*cell == *old) { *cell = *new; return TRUE;}
else { *old = *cell; return FALSE;}
}
```

 After finishing all things, a process call `give()` to wake up the first process in the waiting list

An example `give()` of an exclusive semaphore
![Semaphore-Give](/assets/images/Pasted image 20240417095419.png)

Problem of Exclusive Semaphore: Dead Locks

Solution to dead locks:
- Have enough resources – not practical
- don’t allow a process to wait permanently. simply **rollback** after a certain time
	bad idea too. causing resources waste and repeated deadlocks. 
	it causes *live locks* → constantly change the state of processes but with no actual progress 
- Linearly order the resources and request of resources should follow this order – practical
	guarantees that no cyclic dependencies among the transactions.
	i.e. a transaction after requesting ith resource can request jth resource if j > i.
	it applies to statis resources such as printers, but it generally can’t apply to db in general because you don’t know how to order things until runtime  
- pre-declare all necessary resources and allocate at once
- Periodically check cycles in graphs
	if a cycle exists, rollback one or more transaction
	not common in practical:
	- check if there is a cycle in distributed states is impossible to be absolutely correct 
	- how to decide which transaction to sacrifice?
- Allow waiting for a maximum time on a lock then force Rollback: Many successful systems (IBM, etc) have chosen this approach…
- Most practical approach: allow a transaction waiting for a maximum time on a lock then force rollback 

### Concurrency Control For Isolation

The relation between concurrency control and isolation can be described in two aspects:
- Concurrent transaction leaves the database in the same state as if the transactions were executed separately
- Isolation guarantees consistency, provided each transaction itself is consistent
	No inconsistency arise from allowing concurrency

By the way, why not just achieve isolation by sequentially processing each transaction? Because **efficiency is also important** in DBMS.

Therefore, we need to use concurrency control to support properties of concurrent transactions:
- concurrent execution should not cause programs to malfunction
- concurrent execution should have higher throughput (otherwise abandon it)

### Achieve Concurrency Control in DB

The tool: dependency graphs

![DependencyGraphs](/assets/images/Pasted image 20240417154210.png)

**The dependencies induced by history fragments.** Each graph shows the two transaction nodes _T1_ and _T2_ and the arcs labeled with the object 〈name,version〉 pairs.

A simple observation: read doesn’t cause concurrency issue

Possible Issues:
- Lost Update: Write after write
- Dirty Read: Read after write
- Unrepeatable Read: READ→WRITE→READ]]></content><author><name>Chew Y. Feng</name></author><category term="CourseNote" /><category term="Database" /><summary type="html"><![CDATA[What is under the hood of DB magic]]></summary></entry><entry><title type="html">Note of Alloy Analyzer</title><link href="http://localhost:4000/coursenote/2024/03/31/Alloy-Analyser.html" rel="alternate" type="text/html" title="Note of Alloy Analyzer" /><published>2024-03-31T00:00:00+11:00</published><updated>2024-03-31T22:30:03+11:00</updated><id>http://localhost:4000/coursenote/2024/03/31/Alloy-Analyser</id><content type="html" xml:base="http://localhost:4000/coursenote/2024/03/31/Alloy-Analyser.html"><![CDATA[* TOC
{:toc}

> - Author: chew.y.feng@outlook.com
> - Date: 03/29/2024

## Introduction
This note is a brief summary of [Alloy](https://alloytools.org/tutorials/online/) for SWEN90010, Unimelb. 

Alloy is an analyzer for software modeling. This subject mainly uses Alloy to find holes in security mechanisms. To be more specific, we use Alloy to *prove* properties about the software specification, such as safety and security.
## Propositional Logic
Alloy support basic propositional Logic:
```coq
all city : AustralianCities | Raining[city]
```
It says "for all cities in Australia, they are raining"
### Relation and Predicates
Everything is a relation in Alloy. Sets are just unary relations. e.g.
```coq
Username = {(U0), (U1), (U2)} 
URL = {(UR0), (UR1), (UR2)} 
Password = {(P0), (P1), (P2)}
```

For arity > 1, relations are also sets e.g.
```coq
Factor(x,y,z) — {(x,y,z) | x = y * z}
```

In summary, we have Sets = Relations = Predicates in alloy
### Operators
**Set operators**

![setops](/assets/images/20240331-alloy-setop.png)

**Relation operators**
```
Username = {(U0, U1, U2)} 
Password = {(P0, P1, P2)} 
// cross product
Username->Password = { (U0, P0), (U0, P1), (U0, P2),
					   (U1, P0), (U1, P1), (U1, P2), 
					   (U2, P0), (U2, P1), (U2, P2)}

urlPasswords = {(U0, UR0, P1), (U0, UR1, P2), (U1, UR0, P2)}
myUsername = {(U0)}
myUrl = {(UR1)}
myPassword = {(P2)}

// dot join
myUsername.urlPasswords = { (UR0, P1), (UR1, P2) }

// box join
myUsername.urlPasswords[myUrl] = {(P2)}

// domain restriction
urlPasswords <: myUsername = {(U0, UR0, P1), (U0, UR1, P2) }

// range restriction
urlPasswords :> myPassword = {(U0, UR1, P2), (U1, UR0, P2)}

// override is defined as A ++ B = (A - (domain[B] <: A) + B)
updatedPassword = {(U0, UR0, P3)}
urlPasswords ++ updatedPassword = {(U0, UR0, P3), (U0, UR1, P2), (U1, UR0, P2)}

// cardinalities
#urlPasswords = 3
```
Note that if the relation has an arity of n, the first (n-1) are seen as domain, the last one is the range.

**Propositional Logic Operators** 

![propop](/assets/images/20240331-alloy-propop.png)

**Quantifiers**

![quantifiers](/assets/images/20240331-alloy-quantifiers.png)

The last four can also be used to declare sets.
## Temporal Logic
One major feature of Alloy is that it can reason about *temporal logic* by *temporal operators*. 
### Temporal Logic
Alloy adopts a model-based specification system, in which the system is defined as a ***state machine model***. In an abstract state machine model, the state e
volves over time

![statemachine](/assets/images/20240329164131.png)

We can describe the transition as ***preconditions*** (what the state should satisfy before the transition) and ***postconditions*** (constraints after the transition).

For safety and security properties, we typically want it to hold in all states. Thus, it is called a ***state invariant***.

To prove a state invariant `inv`, we need to use induction:
(a) `inv` holds in the initial state
(b) For each operation `op_i`, if `inv` holds in states, `inv` still holds after the operation `op_i`.


### Language

There are two ways to express things happen in  a timeline:

(a) Use the next state operator `'` **for sets**
```coq
newPassword = updatedPassword = {(UR0, P3)}
user.passwords’ = user.passwords ++ newPassword
```
It says "In the next state, there is one password being updated"

(b) Use temporal logic keywords **for predicates**
```
// after means the predicate holds for next state on the timeline
delete_all[user] => after (no user.passwords)
```

![temporalops](/assets/images/20240329155752.png)
All temporal operators

## Alloy Language
**Signatures** are type declarations.

![sigs](/assets/images/20240329160557.png)
Common Signature Declarations

Example: Passbook is a database that stores the relation of (user, url, password).
```
sig URL {} 
sig Username {} 
sig Password {} 
sig PassBook {var password : Username -> URL -> Password}
```

**Facts** are constraints that are assumed to hold true. They only check initial state by default , unless there are temporal keywords.  
```
fact NoDuplicates
{
	always all pb : PassBook, user : Username, url : URL 
		| lone pb.password[user][url]
}
```
It reads "for all time, all passbooks, users and URLs, there is at most one password for each `(user,url)` pair"

**Predicates** are primarily used to introduce *operations* over a state
```
//Add a password for a new user/url pair
pred add [pb : PassBook, url : URL, user: Username, pwd: Password] {
	no pb.password[user][url]
	pb.password’ = pb.password + (user->url->pwd)
}

//Delete an existing password
pred delete [pb : PassBook, url : URL, user: Username] {
	one pb.password[user][url]
	pb.password’ = pb.password - (user->url->Password)
}
```

**Functions** are named expressions for reuse of code
```
//Return the password for a given user/URL pair
fun lookup [pb: PassBook, url : URL, user : Username] : lone Password {
	pb.password[user][url]
}
```

**Assertions** are constraints are we want to check. We primarily use them to express the safety or security properties
```
// (durability) If we add a new password, 
// then we get this password when we look it up 
assert addWorks {
	all pb : PassBook, url : URL, user : Username, p : Password |
	add [pb, url, user, p] => (after (lookup [pb, url, user]) = p)
}
```
## Model Checking
The advantage of Alloy is that proofs are automated. Users only need to give the constraints, then Alloy would try to find a counterexample. This approach significantly reduces the efforts in writing proofs. But the tradoff is that it cannot guarantee completeness. 

Example:
```
assert lone_password_per_user_url {
	all pb, user, url, pwd, res | 
	 (all user1, url1 | lone pb.password[user1][url1]) and
	 no pb.password[user][url] and
	 pb.password’ = pb.password + (user->url->pwd) =>
	(lone pb.password’[user][url])
}
```]]></content><author><name>Chew Y. Feng</name></author><category term="CourseNote" /><category term="Formal Method" /><summary type="html"><![CDATA[Light Weighted Formal Methods]]></summary></entry><entry><title type="html">Hoare Logic - Decorated Program</title><link href="http://localhost:4000/software-foundations/2024/02/15/Decorated-Program.html" rel="alternate" type="text/html" title="Hoare Logic - Decorated Program" /><published>2024-02-15T00:00:00+11:00</published><updated>2024-02-15T21:47:25+11:00</updated><id>http://localhost:4000/software-foundations/2024/02/15/Decorated%20Program</id><content type="html" xml:base="http://localhost:4000/software-foundations/2024/02/15/Decorated-Program.html"><![CDATA[* TOC
{:toc}

{% raw %}

## Decorated Programs
The aesthetics of Hoare Logic is that it follows the structure of the program itself, which enables a powerful way to reason about a program -- by decoration.

This naive program which subtracts a number is from the PLT series[^1]. 
```plain
{{ True }} ->> // 1
{{ m = m }}
  X := m
		 {{ X = m }} ->>
		 {{ X = m /\ p = p }};
  Z := p;
		 {{ X = m /\ Z = p }} ->>
		 {{ Z - X = p - m }}
  while X ≠ 0 do
		 {{ Z - X = p - m /\ X ≠ 0 }} ->>
		 {{ (Z - 1) - (X - 1) = p - m }}
	Z := Z - 1
		 {{ Z - (X - 1) = p - m }};
	X := X - 1
		 {{ Z - X = p - m }} // 2
  end
{{ Z - X = p - m /\ ¬(X ≠ 0) }} ->>
{{ Z = p - m }}
```

In this example, we showed that the result of the program is indeed `p - m` by a *decorated proof*. Each line accompanies an assertion which states the condition that the program state meets. The mark 1 "narrows down" the assertion, which is discussed in the previous article[^2]. To remind it, we can use a weaker assertion to replace a stronger one to meet the requirement of the proof goal. The *loop invariant* (mark 2) states the condition that the loop satisfies. For assignments, conditions and most other commands that do not involve loops, finding the assertions is fairly easy by following the rules of Hoare Logic. We could see finding loop invariant is the creative part of this art.  

One could prove a decorated proof is indeed logically correct by an almost automated process - a "prover" (proof assistant) could look at each assertion and line of programs to decide if the deduction is correct. The reason why it could be automated is that the rules of Hoare logic are mechanical enough, as discussed in the previous post[^2].
### Simplified Decorated Programs
The straightforward way to formalize a decorated program is to present two assertions to each one command. Just like the classic Hoare logic statement:
$$
{\displaystyle \{P\}C\{Q\}}
$$
But it turns out to be too verbose that even reading it makes people frustrated.  A decorated program which contains two `skip` would look like this:

```
{{P}} ({{P}} skip {{P}}) ; ({{P}} skip {{P}}) {{P}}
```

An obvious observation is that for two sequential commands, the postcondition of the former one is the precondition of the later one. So we could agree that it's sound to remove one of it.

For `skip` command, apparently, the assertion should not change after the execution. So we don't need to provide the precondition. 
> [!NOTE] Discussion
> Why not omit the postconditions instead? Because the preconditions are always provided either from the start of the program or from the previous command.

Similarly, we could only provide postconditions of assignments, as the preconditions could be derived easily.  

Loops `while b do d` can be simplified by omitting the assertion inside the commands because they can be derived from the preconditions and the postconditions.

```
while b do {{ P }} d end {{ Q }}
```

At last, we arrived to the simplified version of decorated programs, which can be implemented in Coq with some notation magic. The full implementation could be found on[^1]. 
## Automated Verification
A decorated program could be translated to a sequence of verification conditions. By reading each line of the program and the related condition, a proof assistant is able to decide whether the condition could be satisfied. 

Let's look at the standard Hoare triple:
$$
{\displaystyle \{P\}C\{Q\}}
$$
The condition is whether the Hoare triple could be proved *valid*. The command $C$ might be composed by a sequence of commands, and they can be verified mechanically.  Formally, a proof assistant would walk recursively into the command $C$ and generated a big conjunction that checks
- *Local consistency* of each command and
- To "bridge the gap" try to narrow down the assertion found in the program and the assertion imposed by the context. This post adopts the notation  `->>` to notate this "narrowing down" from PLT series[^1]. 
	```
	"P->>Q" <-> forall (st: State), P st -> Q st
	```

A decorated command is local consistent with a precondition `P` if
- it is a skip `skip;{Q}` and  `P ->> Q`.
- it is an assignment `X:=a {Q}` and `P ->> Q[X|->a]` 
- it is a condition
	```
	if b then {{P1}} d1 else {{P2}} d2 end {{Q}}
	```
	- `P /\ b ->> P1`
	- `P /\ ~b ->> P2`
	- `post P1 ->> Q`
	- `post P2 ->> Q`
	where `post` is to get the postcondition of a decorated program
- it is a loop
	```
	while b do {{Q}} d end {{R}}
	```
	- `P /\ b ->> Q`
	- `P /\ ~b ->> R`
	- `post d /\ b ->> Q`
	- `post d /\ ~b ->> R`
- if all explicit usages of `->>` are valid

Following these rules, an automated proof program could be developed. 
## Finding loop invariants
Once the outermost precondition and postcondition are chosen, the only left creative part of this "decorated" art is finding the loop invariants. You can't choose an invariant that is too weak (such as `st -> True`), because you'd need it to bridge the following conditions. Similarly, you can't choose an invariant that is too strong, because it would be extremely hard or impossible to prove. It is indeed an art of finding the most appropriate loop invariants. 

There is no silver bullet to solve the problem, but a few tips might help:
- Try to use the strongest condition as the invariant first (the postcondition of the loop), and see what made it fails. Then adjust the invariants to a more "realistic one" 
- Think about what the loop does and, how, by the end of the loop, the assertion `Q /\ b` is useful for proving the postcondition.
- What variables are used in the loop and how to propagate these variables into the information that the invariant carries.

## Summary
- Decorated Programs could be viewed as proofs of programs.
- Simplified Decorated Programs are equivalent to a fully decorated one, and it could help to avoid verbose, and thus, easier to read.
- The verification of a decorated program could be made automated.
- Finding loop invariants is relatively hard in this art.

## Referrecne

[^1]: https://softwarefoundations.cis.upenn.edu/plf-current/Hoare2.html
[^2]: https://excitedspider.github.io/software-foundations/2024/02/02/Hoare-Logic-Basic.html

{% endraw %}]]></content><author><name>Chew Y. Feng</name></author><category term="Software-Foundations" /><category term="Coq" /><category term="PLT" /><summary type="html"><![CDATA[Decoration and Automated Verification]]></summary></entry><entry><title type="html">Hoare Logic Basic</title><link href="http://localhost:4000/software-foundations/2024/02/02/Hoare-Logic-Basic.html" rel="alternate" type="text/html" title="Hoare Logic Basic" /><published>2024-02-02T00:00:00+11:00</published><updated>2024-02-15T21:47:25+11:00</updated><id>http://localhost:4000/software-foundations/2024/02/02/Hoare%20Logic%20Basic</id><content type="html" xml:base="http://localhost:4000/software-foundations/2024/02/02/Hoare-Logic-Basic.html"><![CDATA[* TOC
{:toc}

{% raw %}

The _Floyd-Hoare Logic_, or _Hoare Logic_, is a formal logic system which is able to reason about the correctness of computer programs **rigorously** and **compositionally**. It is rigorous in the sense that it bases on formal logics. It is compositional because it allows researchers to look at the syntactic constructs of a imperative program. 

There are two main ideas of Hoare Logics:
1. writing down formal specifications of programs
2. proof technique of programs which mirrors the structure of program

## Assertions

Assertions are logical claims about the state of programs. Intuitively, programmers want the programs to satisfy some constrains at a certain point.  For example, if I wrote such code

```java
File myObj = new File("filename.txt");
Scanner myReader = new Scanner(myObj);
String data = myReader.nextLine(); // <-
```

Before execution of the last line, I certainly want there are lines to read. In other words, it should be in a state that  `myReader` is valid:

```java
() -> this.myReader.hasNextLine()
```

Formally, an assertion is a property of states, which states the correctness of a program.

```coq
Definition Assertion := state → Prop.
```

## Hoare Triple
The central of Hoare Logic is the *Hoare Triple*. A triple describes how the execution of a piece of code changes the state ([program state](https://en.wikipedia.org/wiki/State_(computer_science))). The formal notation of a triple is:

$$
\{P\}~c~\{Q\}
$$

where P and Q are assertions, c is the piece of code (commands) that being executed. P is named the _precondition_ and Q the _postcondition_. This triple asserts that if the state satisfy $P$, after code $c$ was executed, the state should satisfy $Q$. 

Reasoning about the program is (partially) equivalent to see if a triple is _valid_. For example, the triple

```coq
{ X = 0 } X := X + 1 {X = 1}
```

is apparently valid. (Here I adopt the annotation of assertions from [^1])

We can formalize the validity of triples as a proposition

```coq
Definition valid_hoare_triple
           (P : Assertion) (c : com) (Q : Assertion) : Prop :=
  forall st st',
     st =[ c ]=> st' ->
     P st  ->
     Q st'.
```

## Structured Proof Rules

A wonderful idea of Hoare Logic is that the proof mirrors the structure of the program itself. Let's look at some fundamental ones based on the language imp (which defined in [^1]). 

### Sequencing

A sequence of commands can be "torn down" by look at each command.  The rule is:

$$
{\displaystyle {\dfrac {\{P\}S\{Q\}\quad ,\quad \{Q\}T\{R\}}{\{P\}S;T\{R\}}}}
$$

which can be easily translated to Coq:

```coq
Theorem hoare_seq : ∀ P Q R c1 c2,
     {{Q}} c2 {{R}} →
     {{P}} c1 {{Q}} →
     {{P}} c1; c2 {{R}}.
```

Although it is intuitive, but one should see that it shows how Hoare Logic can **compositionally** reason a program - by the transitivity of triples - without taking the program as a whole. 
### Assignment

For imperative programs, the typical command that changes the program state is assignment. Thus, it is the most fundamental of the Hoare logic.

$$
{\dfrac {}{\{P[E/x]\}x:=E\{P\}}}
$$

where $P[E/x]$ means the assertion $P$ in which every free occurrence of x as been replaced by E.    

For example, what the precondition could be in this triple?

$$\{ ??? \}~X:= X + Y~\{X=1\}$$

If we replace $X$ with $X+Y$ in the postcondition, a valid precondition is acquired. 

$$\{ X+Y = 1 \}~X:= X + Y~\{X=1\}$$

This trick works because it exploits the definition of assignment. Besides, this trick is so mechanical that it could be implemented in Coq trivially.

The implementation of $P[E/x]$:

```coq
Definition assertion_sub X a (P:Assertion) : Assertion :=
  fun (st : state) =>
    P (X !-> aeval st a ; st).
```

This function takes a variable `X`, an expression `a` and an assertion P, returns an assertion that replaces the evaluation of `st X` to `st a`.

It is also known as "backwards reasoning" because we take postconditions and commands as inputs, outputing the preconditions. Actually it is a good way to think because the postconditions are usually the most important. But it is not to say that it's impossible to do forward reasoning - see [^2] for more examples.

To translate this rule into Coq:

```coq
Theorem hoare_asgn : ∀ Q X a,
  {{Q [X |-> a]}} X := a {{Q}}.

Example hoare_asgn_examples1 :
  exists P,
    {{ P }}
      X := 2 * X
    {{ X <= 10 }}.
Proof.
  exists ((X <= 10) [X |-> 2 * X]).
  apply hoare_asgn.
Qed.
```

A common error is to use this rule in a "forward" way. In other words, this is incorrect: ${\displaystyle \{P\}x:=E\{P[E/x]\}}$

We give a counterexample of it to show that it is indeed incorrect:

$${\displaystyle \{\text{True}\}~x:=a~\{X = a\}}$$

```coq
Theorem hoare_asgn_wrong : exists a:aexp,
  ~ {{ True }} X := a {{ X = a }}.
Proof.
  exists (APlus (AId X) (ANum 1)). (* X := X + 1 *)
  unfold not.
  unfold valid_hoare_triple.
  intros.
  remember (X !-> 0) as st.
  remember (X !-> 1; st) as st'.
  unfold Aexp_of_aexp in H.
  assert (aeval st' X = aeval st' <{ X + 1 }>).
  {
    apply H with st.
    - subst.
      apply E_Asgn.
      reflexivity.
    - unfold assert_of_Prop. trivial.
  }
  subst. inversion H0.
Qed.
```

### Consequence

$${\displaystyle {\dfrac {P_{1}\rightarrow P_{2}\quad ,\quad \{P_{2}\}S\{Q_{2}\}\quad ,\quad Q_{2}\rightarrow Q_{1}}{\{P_{1}\}S\{Q_{1}\}}}}$$

This rule allows strengthening the precondition or(and) weaken the postconditions.

For example, if my precondition is $\{ X > 10 \}$ , I can strengthen it to $\{X > 20\}$. For most of the time, this rule is used to adjust the assertions to what we need.

### Conditions
A conditional command `if b then s else t end` naturally has this form in logic: 

$${\displaystyle {\dfrac {\{B\wedge P\}S\{Q\}\quad ,\quad \{\neg B\wedge P\}T\{Q\}}{\{P\}{\texttt {if}}\ B\ {\texttt {then}}\ S\ {\texttt {else}}\ T\ {\texttt {end}}\{Q\}}}}$$

That is, in the `then` branch, we know $B$ is evaluated to $\text{True}$; and in the `else` branch, $B$ is evaluated to $\text{False}$.  We could use this information in reasoning. Moreover, if $B$ (or $\lnot B$) contradicts $P$, we could rule out that branch because its hypothesis contains a contradiction. 

Formalization in coq:

```coq
Theorem hoare_if : forall P Q (b:bexp) c1 c2,
  {{ P /\ b }} c1 {{Q}} ->
  {{ P /\ ~ b}} c2 {{Q}} ->
  {{P}} if b then c1 else c2 end {{Q}}.

(* example *)
Theorem if_minus_plus :
  {{True}}
    if (X <= Y)
      then Z := Y - X
      else Y := X + Z
    end
  {{Y = X + Z}}.
Proof.
  apply hoare_if; eapply hoare_consequence_pre;
  try (apply hoare_asgn); simpl; assertion_auto''.
Qed.
```

### Loops

$${\displaystyle {\dfrac {\{P\wedge B\}S\{P\}}{\{P\}{\texttt {while}}\ B\ {\texttt {do}}\ S\ {\texttt {end}}\{\neg B\wedge P\}}}}$$

This rule captures the most important behaviors of loops:
- The loop body will be executed only if $B$ is true
- The loop terminates when $B$ becomes false.
- We call $P$ a loop invariant to $S$ if $\\{P \wedge B\\}S\\{P\\}$. This means P will be true at the end of the loop body if $B$ is true in the beginning. Otherwise, (if P contradicts B), it still holds because the precondition becomes trivial.

Note that Hoare Logic only cares about loops that terminates. In other word, only partial correctness can be proven. There is a variant version of loop rule that can be formulated to prove total correctness[^2].  But as deciding if a program halts is known as undecidable, it is hard to work with total correctness, or it would require special designs of programming languages.

Formalize it in Coq

```coq
Theorem hoare_while : forall P (b:bexp) c,
  {{P /\ b}} c {{P}} ->
  {{P}} while b do c end {{P /\ ~ b}}.
```
## Summary
- Assertions are properties of program states.
- Hoare Triples are built on assertions.
- Hoare Triples can be reasoned mechanically through proof rules 

## Reference

[^1]: [PROGRAMMING LANGUAGE FOUNDATIONS](https://softwarefoundations.cis.upenn.edu/plf-current/index.html)
[^2]: [Wikipedia: Hoare logic](https://en.wikipedia.org/wiki/Hoare_logic)

{% endraw %}]]></content><author><name>Chew Y. Feng</name></author><category term="Software-Foundations" /><category term="Coq" /><category term="PLT" /><summary type="html"><![CDATA[Assertions, Hoare Triples and Proof Rules.]]></summary></entry><entry><title type="html">Program Equivalence</title><link href="http://localhost:4000/software-foundations/2024/01/25/Program-Equivalence.html" rel="alternate" type="text/html" title="Program Equivalence" /><published>2024-01-25T00:00:00+11:00</published><updated>2024-01-25T18:36:38+11:00</updated><id>http://localhost:4000/software-foundations/2024/01/25/Program-Equivalence</id><content type="html" xml:base="http://localhost:4000/software-foundations/2024/01/25/Program-Equivalence.html"><![CDATA[* TOC
{:toc}

This is a summary for the programming language foundation, chapter "Program Equivalence"  [^1]. 

## Behavioral Equivalence
The concept of ***equivalence*** is crucial to understand what is a correct program. For an example, if  program $A$ is correct and $B$ is equivalent to $A$, then it is guaranteed that $B$ is correct. 

Generally, an equivalence is a relation that is *reflective*, *symmetric* and *transitive*. So there are many ways to define equivalence. I could define an equivalence relation as two programs are identical as two strings after remove whitespaces. But this definition is not so interesting because it is not helpful for us to improve current programs. Instead, a more useful definition is *behavioral equivalence*. 

Two programs are said to be *behaviorally equivalent* if they evaluate to the same result from every state.

```coq
Definition cequiv (c1 c2 : com) : Prop :=
  forall (st st' : state),
    (st =[ c1 ]=> st') <-> (st =[ c2 ]=> st').
```

(The language we are working on is called "imp", which is a simple language that has the most basic and representative features as general purpose imperative languages. see [imp](https://softwarefoundations.cis.upenn.edu/lf-current/Imp.html)  .)

We can prove that adding a `skip` results in an equivalent program.

```coq
Theorem skip_right : forall c,
  cequiv
    <{ c ; skip }>
    c.
Proof.
  intros. 
  split; intros H.
  - inversion H. subst. inversion H5. subst. assumption.
  - apply E_Seq with (st':=st').
    + assumption.
    + apply E_Skip.
Qed.  
```

This definition implies an important concerns: if the program would terminate. 

Programmers also often say that program c1 *refines* c2. It is an asymmetric variant of equivalence - c1 produces the same final states when it terminates - and perhaps c1 is better in readability or  efficiency. But it is not guaranteed that c1 terminates on some initial states if c2 does.

```coq
Definition refines (c1 c2 : com) : Prop :=
  forall (st st' : state),
    (st =[ c1 ]=> st') -> (st =[ c2 ]=> st').
```

### Conditions

if the predicate is always false, it's safe remove the truth branch.

```coq
Theorem if_false : forall b c1 c2,
  bequiv b <{false}> ->
  cequiv
    <{ if b then c1 else c2 end }>
    c2.
Proof.
  intros.
  split; intros H1.
  - inversion H1; subst.
    + unfold bequiv in H. simpl in H.
      rewrite H in H6. discriminate H6.
    + assumption.
  - apply E_IfFalse; try (assumption). 
    unfold bequiv in H. simpl in H. rewrite H. trivial.
Qed.
```

### Loops

In a loop, if a predicate is always true, the program would never terminate.

```coq
Lemma while_true_nonterm : ∀ b c st st',
  bequiv b <{true}> →
  ~( st =[ while b do c end ]=> st' ).
```

(This is not halting problem if you are thinking about that because it doesn't decide every input.)

Thus, two programs are equivalent if the are both infinite loops, regardless their body. 

```coq
Theorem while_true : forall b c,
  bequiv b <{true}>  ->
  cequiv
    <{ while b do c end }>
    <{ while true do skip end }>.
Proof.
  intros. 
  split; intros H0.
  - inversion H0; subst.
    + unfold bequiv in H. rewrite H in H5. discriminate.
    + apply (while_true_nonterm b c st st') in H. contradiction.
  - exfalso. 
    apply (while_true_nonterm <{true}> <{skip}> st st'); 
    try (unfold bequiv);
    try (auto).
Qed.
```

Conversely, if the predicate is always false, it's safe to optimize the program by removing the surrounding while.

```coq
Theorem if_false : forall b c1 c2,
  bequiv b <{false}> ->
  cequiv
    <{ if b then c1 else c2 end }>
    c2.
Proof.
  intros.
  split; intros H1.
  - inversion H1; subst.
    + unfold bequiv in H. simpl in H.
      rewrite H in H6. discriminate H6.
    + assumption.
  - apply E_IfFalse; try (assumption). 
    unfold bequiv in H. simpl in H. rewrite H. trivial.
Qed.
```

### Assignments

An assignment is redundant is assign to the same value that the variable currently holds.

```coq
Theorem assign_aequiv : forall (X : string) (a : aexp),
  aequiv <{ X }> a ->
  cequiv <{ skip }> <{ X := a }>.
Proof.
  unfold aequiv.
  intros. split; intros H0.
  - inversion H0; subst.
    assert (Hx: st' =[ X:=a ]=> ( X !-> st' X; st')).
      { apply E_Asgn. rewrite <- H. reflexivity. }
    rewrite t_update_same in Hx. assumption.
  - inversion H0; subst.
    assert (Hxa: (X !-> aeval st a; st) = (X !-> aeval st X; st)).
      { rewrite H. reflexivity. }
    rewrite Hxa. rewrite t_update_same. auto using (E_Skip).
Qed.
```

## Behavioral Equivalence is Congruence

The behavior equivalence is also a *congruence*. Informally, it is a congruence in the sense that two subprograms implies the equivalence of the larger programs. For example:

$$c_1 = c_1' \implies c2 = c2' \implies (c_1;c_2) = (c_1';c_2')$$

This fact is important in the sense that it allows us to replace a small part of a large program by an equivalent subprogram without proving those parts that didn't change.   

Formalize the idea of congruence into Coq needs some work. The most basic one is the sequence commands.

```coq
Theorem CSeq_congruence : forall c1 c1' c2 c2',
  cequiv c1 c1' -> cequiv c2 c2' ->
  cequiv <{ c1;c2 }> <{ c1';c2' }>.
Proof.
    intros. unfold cequiv in *. split; intros Hequiv.
    + inversion Hequiv; subst.
      rewrite H in *. rewrite H0 in *.
      apply E_Seq with (st'0); assumption.
    + inversion Hequiv; subst.
      rewrite <- H in *. rewrite <- H0 in *.
      apply E_Seq with (st'0); assumption.
Qed.
```

There are some equivalence relations that are not congruence.

Define equivalence: two programs are said to be equivalent if after applying to an empty state (that is, no variables in record), the numbers of variables are the same. It is easy to see this relation is reflective, symmetric and transitive. But the relation is not congruence. Proof:

Consider two programs: (x:=0) = (y:=1)

It is not a congruence because

1. (x:=0) = (y:=1)
2. (x:=1) = (x:=1)

but 

3. (x:=0;x:=1) != (y:=1;x:=1)

QED.

## Program Transformation

A program transformation is a function that takes one program as input and produces a modified program. Compilers are the most popular kind of transformation. [Minifiers](https://en.wikipedia.org/wiki/Minification_(programming))and [obfuscator](https://en.wikipedia.org/wiki/Obfuscation_(software)) are also useful in many areas. 

A practical application for equivalence relation is to show that ana transformation is *sound*, that is, the input and output program are equivalent. It is important to do so if some optimization are applied to the transformation. 

Soundness is formalize as a property on a transformation:

```coq
Definition ctrans_sound (ctrans : com -> com) : Prop :=
  forall (c : com),
    cequiv c (ctrans c).
```

For instance, there is a common optimization in compilers called [constant-folding](https://en.wikipedia.org/wiki/Constant_folding). (the implementation of constant-folding can be found in the programming language foundation book, see reference [^1]).

```coq
Theorem fold_constants_com_sound :
  ctrans_sound fold_constants_com.
Proof.
  unfold ctrans_sound. intros c.
  induction c; simpl.
  - (* skip *) apply refl_cequiv.
  - (* := *) apply CAsgn_congruence.
              apply fold_constants_aexp_sound.
  - (* ; *) apply CSeq_congruence; assumption.
  - (* if *)
    assert (bequiv b (fold_constants_bexp b)). {
      apply fold_constants_bexp_sound. }
    destruct (fold_constants_bexp b) eqn:Heqb;
      try (apply CIf_congruence; assumption).
      (* (If the optimization doesn't eliminate the if, then the
          result is easy to prove from the IH and
          [fold_constants_bexp_sound].) *)
    + (* b always true *)
      apply trans_cequiv with c1; try assumption.
      apply if_true; assumption.
    + (* b always false *)
      apply trans_cequiv with c2; try assumption.
      apply if_false; assumption.
  - assert (bequiv b (fold_constants_bexp b)).
    { apply fold_constants_bexp_sound. }
    destruct (fold_constants_bexp b);
    try (apply CWhile_congruence; assumption).
    + apply while_true. assumption.
    + apply while_false. assumption.
Qed.
```


## References
[^1]: “Program Equivalence.” 2024. _Equiv: Program Equivalence_. Accessed January 25. https://softwarefoundations.cis.upenn.edu/plf-current/Equiv.html.]]></content><author><name>Chew Y. Feng</name></author><category term="Software-Foundations" /><category term="Coq" /><category term="PLT" /><summary type="html"><![CDATA[Formalize equivalence relations.]]></summary></entry></feed>